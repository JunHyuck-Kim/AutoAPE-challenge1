{"cells":[{"metadata":{"_uuid":"9e97164a7191d9d43ba8158c0f7e13aa455a0ce9"},"cell_type":"markdown","source":"# Combining your model with a model without outlier"},{"metadata":{"_uuid":"e0f082732acbf893c2cd4c07200904effcfde928"},"cell_type":"markdown","source":"feature engineering을 끝내고 나면 두가지 데이터셋이 남아 있음\n\n- ***train_clean.csv***\n- ***test_clean.csv***\n이상치 다루는 게 핵심인 커널\n\nThe flows of this pipline is as follows:\n1. Training a model using a training set without outliers. (we get: **Model_1**)\n2. Training a model to classify outliers. (we get: **Model_2**)\n3. Using **Model_2** to predict whether an card_id in test set is an outliers. (we get:**Outlier_Likelyhood**)\n4. Spliting out the card_id from **Outlier_Likelyhood** with top 10% (or some other ratio) score. (we get:**Outlier_ID**)\n5. Combining your submission using your **best submission (that is, your best model)** to predict **Outlier_ID** in test set and using **Model_1** to predict the rest of the test set.\n>>> \n핵심 아이디어 \n1. Training model without outliers make the model more accurate for non-outliers.\n2. A great proportion of the error is caused by outliers, so we need to use a model training with outliers to predict them. How to find them out? build a classifier!"},{"metadata":{"trusted":true,"_uuid":"7dd3ea3236069cf9706e377d6594c39b5e5a5507"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import log_loss","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"fcf141a7658f4089b1de2822ba41da9646c73835"},"cell_type":"markdown","source":"# Part 1 Training Model Without Outliers"},{"metadata":{"trusted":true,"_uuid":"aef67435b765a8c55e8a7653b47ad8d9071ea11b"},"cell_type":"code","source":"%%time\ndf_train = pd.read_csv('../input/predicting-outliers-to-improve-your-score/train_clean.csv')\ndf_test = pd.read_csv('../input/predicting-outliers-to-improve-your-score/test_clean.csv')","execution_count":2,"outputs":[{"output_type":"stream","text":"CPU times: user 7.63 s, sys: 1.15 s, total: 8.77 s\nWall time: 8.79 s\n","name":"stdout"}]},{"metadata":{"_uuid":"18d93d315eee398fb1cff4ad8d115c21d359cd49"},"cell_type":"markdown","source":"## filtering out outliers"},{"metadata":{"trusted":true,"_uuid":"3eb5ac784a25271b3433f6371d7b46cdabb420b5"},"cell_type":"code","source":"df_train = df_train[df_train['outliers'] == 0]\ntarget = df_train['target']\ndel df_train['target']\nfeatures = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','outliers']]\ncategorical_feats = [c for c in features if 'feature_' in c]","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"8256bfde460ac4adc45852228a12194b2be6b497"},"cell_type":"markdown","source":"## parameters"},{"metadata":{"trusted":true,"_uuid":"a0ecb4a6605d477b290275f404a508bfacd614e0"},"cell_type":"code","source":"param = {'objective':'regression',\n         'num_leaves': 31,\n         'min_data_in_leaf': 25,\n         'max_depth': 7,\n         'learning_rate': 0.01,\n         'lambda_l1':0.13,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\":0.85,\n         'bagging_freq':8,\n         \"bagging_fraction\": 0.9 ,\n         \"metric\": 'rmse',\n         \"verbosity\": -1,\n         \"random_state\": 2333}","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"204231995912b388ad8d6bed1c602c6fa29ebb4a"},"cell_type":"markdown","source":"## training model"},{"metadata":{"trusted":true,"_uuid":"1f153534282a64ea904b599143b023f56bf01e05","scrolled":true},"cell_type":"code","source":"%%time\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2333)\noof = np.zeros(len(df_train))\npredictions = np.zeros(len(df_test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train,df_train['outliers'].values)):\n    print(\"fold {}\".format(fold_))\n    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval= 100, early_stopping_rounds = 200)\n    oof[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(df_test[features], num_iteration=clf.best_iteration) / folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))","execution_count":5,"outputs":[{"output_type":"stream","text":"fold 0\nTraining until validation scores don't improve for 200 rounds.\n[100]\ttraining's rmse: 1.60695\tvalid_1's rmse: 1.61793\n[200]\ttraining's rmse: 1.57615\tvalid_1's rmse: 1.59127\n[300]\ttraining's rmse: 1.5615\tvalid_1's rmse: 1.58071\n[400]\ttraining's rmse: 1.55208\tvalid_1's rmse: 1.57484\n[500]\ttraining's rmse: 1.54492\tvalid_1's rmse: 1.57124\n[600]\ttraining's rmse: 1.53916\tvalid_1's rmse: 1.5691\n[700]\ttraining's rmse: 1.53428\tvalid_1's rmse: 1.56768\n[800]\ttraining's rmse: 1.52983\tvalid_1's rmse: 1.56669\n[900]\ttraining's rmse: 1.52574\tvalid_1's rmse: 1.56596\n[1000]\ttraining's rmse: 1.5219\tvalid_1's rmse: 1.56532\n[1100]\ttraining's rmse: 1.51842\tvalid_1's rmse: 1.56506\n[1200]\ttraining's rmse: 1.51513\tvalid_1's rmse: 1.56475\n[1300]\ttraining's rmse: 1.51195\tvalid_1's rmse: 1.56444\n[1400]\ttraining's rmse: 1.50873\tvalid_1's rmse: 1.56409\n[1500]\ttraining's rmse: 1.50566\tvalid_1's rmse: 1.56382\n[1600]\ttraining's rmse: 1.5024\tvalid_1's rmse: 1.56365\n[1700]\ttraining's rmse: 1.49932\tvalid_1's rmse: 1.56348\n[1800]\ttraining's rmse: 1.49619\tvalid_1's rmse: 1.56336\n[1900]\ttraining's rmse: 1.49337\tvalid_1's rmse: 1.56322\n[2000]\ttraining's rmse: 1.49055\tvalid_1's rmse: 1.56316\n[2100]\ttraining's rmse: 1.48758\tvalid_1's rmse: 1.56319\n[2200]\ttraining's rmse: 1.48459\tvalid_1's rmse: 1.56311\n[2300]\ttraining's rmse: 1.48164\tvalid_1's rmse: 1.56303\n[2400]\ttraining's rmse: 1.47897\tvalid_1's rmse: 1.56298\n[2500]\ttraining's rmse: 1.4762\tvalid_1's rmse: 1.56287\n[2600]\ttraining's rmse: 1.47367\tvalid_1's rmse: 1.56283\n[2700]\ttraining's rmse: 1.47072\tvalid_1's rmse: 1.56279\n[2800]\ttraining's rmse: 1.46792\tvalid_1's rmse: 1.56275\n[2900]\ttraining's rmse: 1.46525\tvalid_1's rmse: 1.56258\n[3000]\ttraining's rmse: 1.46263\tvalid_1's rmse: 1.56247\n[3100]\ttraining's rmse: 1.46008\tvalid_1's rmse: 1.56244\n[3200]\ttraining's rmse: 1.45738\tvalid_1's rmse: 1.56232\n[3300]\ttraining's rmse: 1.45488\tvalid_1's rmse: 1.56232\n[3400]\ttraining's rmse: 1.45225\tvalid_1's rmse: 1.56225\n[3500]\ttraining's rmse: 1.44938\tvalid_1's rmse: 1.56218\n[3600]\ttraining's rmse: 1.44677\tvalid_1's rmse: 1.56216\n[3700]\ttraining's rmse: 1.44437\tvalid_1's rmse: 1.5622\n[3800]\ttraining's rmse: 1.44165\tvalid_1's rmse: 1.56229\nEarly stopping, best iteration is:\n[3623]\ttraining's rmse: 1.44628\tvalid_1's rmse: 1.56214\nfold 1\nTraining until validation scores don't improve for 200 rounds.\n[100]\ttraining's rmse: 1.61228\tvalid_1's rmse: 1.59563\n[200]\ttraining's rmse: 1.58153\tvalid_1's rmse: 1.56933\n[300]\ttraining's rmse: 1.5667\tvalid_1's rmse: 1.55847\n[400]\ttraining's rmse: 1.55704\tvalid_1's rmse: 1.553\n[500]\ttraining's rmse: 1.55004\tvalid_1's rmse: 1.54988\n[600]\ttraining's rmse: 1.54427\tvalid_1's rmse: 1.54823\n[700]\ttraining's rmse: 1.5395\tvalid_1's rmse: 1.54685\n[800]\ttraining's rmse: 1.53503\tvalid_1's rmse: 1.546\n[900]\ttraining's rmse: 1.53072\tvalid_1's rmse: 1.54544\n[1000]\ttraining's rmse: 1.52696\tvalid_1's rmse: 1.54505\n[1100]\ttraining's rmse: 1.52323\tvalid_1's rmse: 1.54474\n[1200]\ttraining's rmse: 1.51975\tvalid_1's rmse: 1.54443\n[1300]\ttraining's rmse: 1.51641\tvalid_1's rmse: 1.5443\n[1400]\ttraining's rmse: 1.51311\tvalid_1's rmse: 1.54417\n[1500]\ttraining's rmse: 1.50977\tvalid_1's rmse: 1.54411\n[1600]\ttraining's rmse: 1.50666\tvalid_1's rmse: 1.54403\n[1700]\ttraining's rmse: 1.50355\tvalid_1's rmse: 1.54388\n[1800]\ttraining's rmse: 1.50049\tvalid_1's rmse: 1.5438\n[1900]\ttraining's rmse: 1.49767\tvalid_1's rmse: 1.54391\nEarly stopping, best iteration is:\n[1796]\ttraining's rmse: 1.50061\tvalid_1's rmse: 1.54379\nfold 2\nTraining until validation scores don't improve for 200 rounds.\n[100]\ttraining's rmse: 1.60514\tvalid_1's rmse: 1.62645\n[200]\ttraining's rmse: 1.57462\tvalid_1's rmse: 1.59821\n[300]\ttraining's rmse: 1.56025\tvalid_1's rmse: 1.58673\n[400]\ttraining's rmse: 1.5508\tvalid_1's rmse: 1.58061\n[500]\ttraining's rmse: 1.54386\tvalid_1's rmse: 1.57693\n[600]\ttraining's rmse: 1.5384\tvalid_1's rmse: 1.57475\n[700]\ttraining's rmse: 1.53341\tvalid_1's rmse: 1.57317\n[800]\ttraining's rmse: 1.52902\tvalid_1's rmse: 1.57207\n[900]\ttraining's rmse: 1.52505\tvalid_1's rmse: 1.57149\n[1000]\ttraining's rmse: 1.52124\tvalid_1's rmse: 1.57094\n[1100]\ttraining's rmse: 1.51771\tvalid_1's rmse: 1.57062\n[1200]\ttraining's rmse: 1.51402\tvalid_1's rmse: 1.57026\n[1300]\ttraining's rmse: 1.51068\tvalid_1's rmse: 1.56987\n[1400]\ttraining's rmse: 1.5073\tvalid_1's rmse: 1.56966\n[1500]\ttraining's rmse: 1.50415\tvalid_1's rmse: 1.56953\n[1600]\ttraining's rmse: 1.50101\tvalid_1's rmse: 1.56937\n[1700]\ttraining's rmse: 1.49813\tvalid_1's rmse: 1.56932\n[1800]\ttraining's rmse: 1.49526\tvalid_1's rmse: 1.56922\n[1900]\ttraining's rmse: 1.49227\tvalid_1's rmse: 1.56916\n[2000]\ttraining's rmse: 1.48924\tvalid_1's rmse: 1.56901\n[2100]\ttraining's rmse: 1.48636\tvalid_1's rmse: 1.56893\n[2200]\ttraining's rmse: 1.48352\tvalid_1's rmse: 1.56891\n[2300]\ttraining's rmse: 1.4805\tvalid_1's rmse: 1.569\n[2400]\ttraining's rmse: 1.4776\tvalid_1's rmse: 1.56918\nEarly stopping, best iteration is:\n[2234]\ttraining's rmse: 1.48249\tvalid_1's rmse: 1.56885\nfold 3\nTraining until validation scores don't improve for 200 rounds.\n[100]\ttraining's rmse: 1.60992\tvalid_1's rmse: 1.60214\n[200]\ttraining's rmse: 1.57897\tvalid_1's rmse: 1.57736\n[300]\ttraining's rmse: 1.56412\tvalid_1's rmse: 1.5678\n[400]\ttraining's rmse: 1.55454\tvalid_1's rmse: 1.56257\n[500]\ttraining's rmse: 1.54745\tvalid_1's rmse: 1.55952\n[600]\ttraining's rmse: 1.54179\tvalid_1's rmse: 1.55762\n[700]\ttraining's rmse: 1.53687\tvalid_1's rmse: 1.55628\n[800]\ttraining's rmse: 1.53254\tvalid_1's rmse: 1.55549\n[900]\ttraining's rmse: 1.52855\tvalid_1's rmse: 1.55494\n[1000]\ttraining's rmse: 1.52469\tvalid_1's rmse: 1.5546\n[1100]\ttraining's rmse: 1.52106\tvalid_1's rmse: 1.5543\n[1200]\ttraining's rmse: 1.51738\tvalid_1's rmse: 1.55421\n[1300]\ttraining's rmse: 1.51403\tvalid_1's rmse: 1.55399\n[1400]\ttraining's rmse: 1.51094\tvalid_1's rmse: 1.55379\n[1500]\ttraining's rmse: 1.50774\tvalid_1's rmse: 1.55365\n[1600]\ttraining's rmse: 1.5047\tvalid_1's rmse: 1.55361\n[1700]\ttraining's rmse: 1.50158\tvalid_1's rmse: 1.55351\n[1800]\ttraining's rmse: 1.4985\tvalid_1's rmse: 1.55339\n[1900]\ttraining's rmse: 1.4956\tvalid_1's rmse: 1.5534\nEarly stopping, best iteration is:\n[1777]\ttraining's rmse: 1.49931\tvalid_1's rmse: 1.55332\nfold 4\nTraining until validation scores don't improve for 200 rounds.\n[100]\ttraining's rmse: 1.60698\tvalid_1's rmse: 1.61912\n[200]\ttraining's rmse: 1.57646\tvalid_1's rmse: 1.59206\n[300]\ttraining's rmse: 1.56182\tvalid_1's rmse: 1.58069\n[400]\ttraining's rmse: 1.55237\tvalid_1's rmse: 1.57491\n[500]\ttraining's rmse: 1.54527\tvalid_1's rmse: 1.57141\n[600]\ttraining's rmse: 1.53978\tvalid_1's rmse: 1.56911\n[700]\ttraining's rmse: 1.53479\tvalid_1's rmse: 1.56767\n[800]\ttraining's rmse: 1.53043\tvalid_1's rmse: 1.56664\n[900]\ttraining's rmse: 1.52643\tvalid_1's rmse: 1.56584\n[1000]\ttraining's rmse: 1.52257\tvalid_1's rmse: 1.56539\n[1100]\ttraining's rmse: 1.51908\tvalid_1's rmse: 1.56487\n[1200]\ttraining's rmse: 1.51554\tvalid_1's rmse: 1.56449\n[1300]\ttraining's rmse: 1.51219\tvalid_1's rmse: 1.56436\n[1400]\ttraining's rmse: 1.50879\tvalid_1's rmse: 1.56412\n[1500]\ttraining's rmse: 1.50579\tvalid_1's rmse: 1.5639\n[1600]\ttraining's rmse: 1.50276\tvalid_1's rmse: 1.56368\n[1700]\ttraining's rmse: 1.49957\tvalid_1's rmse: 1.56354\n[1800]\ttraining's rmse: 1.49645\tvalid_1's rmse: 1.56344\n[1900]\ttraining's rmse: 1.49359\tvalid_1's rmse: 1.56327\n[2000]\ttraining's rmse: 1.4906\tvalid_1's rmse: 1.56303\n[2100]\ttraining's rmse: 1.48767\tvalid_1's rmse: 1.56305\n[2200]\ttraining's rmse: 1.48468\tvalid_1's rmse: 1.56309\nEarly stopping, best iteration is:\n[2038]\ttraining's rmse: 1.48945\tvalid_1's rmse: 1.56295\nCV score: 1.55824 \nCPU times: user 38min 15s, sys: 11 s, total: 38min 26s\nWall time: 9min 57s\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"4842069eb5b3fa8617b624c1d2fd00940c75137e"},"cell_type":"code","source":"model_without_outliers = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\nmodel_without_outliers[\"target\"] = predictions","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"60b9a679754a23f8a07a4e06bf2003abdfbe57db"},"cell_type":"markdown","source":"# Part 2 Training Model For Outliers Classification"},{"metadata":{"trusted":true,"_uuid":"3f28c4be6b16f4e9d9e66d309a1c8f00175653f1"},"cell_type":"code","source":"%%time\ndf_train = pd.read_csv('../input/predicting-outliers-to-improve-your-score/train_clean.csv')\ndf_test = pd.read_csv('../input/predicting-outliers-to-improve-your-score/test_clean.csv')","execution_count":7,"outputs":[{"output_type":"stream","text":"CPU times: user 7.54 s, sys: 202 ms, total: 7.74 s\nWall time: 7.74 s\n","name":"stdout"}]},{"metadata":{"_uuid":"c2410c958c49b761f042ecdf5619af54730865d3"},"cell_type":"markdown","source":"## using outliers column as labels instead of target column"},{"metadata":{"trusted":true,"_uuid":"19cf3932dbbbc42a8e7e0c50ac7ae4c74fd15f62"},"cell_type":"code","source":"target = df_train['outliers']\ndel df_train['outliers']\ndel df_train['target']","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a60cc387b88592ae2ae6ad78f33d6adcce22780d"},"cell_type":"code","source":"features = [c for c in df_train.columns if c not in ['card_id', 'first_active_month']]\ncategorical_feats = [c for c in features if 'feature_' in c]","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"8867a7ba6c07d9a6c0ee04277960ff4d9c1559a8"},"cell_type":"markdown","source":"## parameters"},{"metadata":{"trusted":true,"_uuid":"b067562422807b5f08688c9c91e22c86998b07c8"},"cell_type":"code","source":"param = {'num_leaves': 31,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 6,\n         'learning_rate': 0.01,\n         \"boosting\": \"rf\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'binary_logloss',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         \"random_state\": 2333}","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"7664c74d5ed84d82d0a045b5f2862cf813a52fd8"},"cell_type":"markdown","source":"## training model"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"292376354fc58ca097cc04128d31f3117c648653"},"cell_type":"code","source":"%%time\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(df_train))\npredictions = np.zeros(len(df_test))\nfeature_importance_df = pd.DataFrame()\n\nstart = time.time()\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, target.values)):\n    print(\"fold n°{}\".format(fold_))\n    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n    oof[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(df_test[features], num_iteration=clf.best_iteration) / folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(log_loss(target, oof)))","execution_count":11,"outputs":[{"output_type":"stream","text":"fold n°0\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1186: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:752: UserWarning: categorical_feature in param dict is overridden.\n  warnings.warn('categorical_feature in param dict is overridden.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 200 rounds.\n[100]\ttraining's binary_logloss: 0.0450977\tvalid_1's binary_logloss: 0.0477188\n[200]\ttraining's binary_logloss: 0.0450068\tvalid_1's binary_logloss: 0.0475699\n[300]\ttraining's binary_logloss: 0.0449617\tvalid_1's binary_logloss: 0.0475141\n[400]\ttraining's binary_logloss: 0.0449697\tvalid_1's binary_logloss: 0.0475231\n[500]\ttraining's binary_logloss: 0.0449932\tvalid_1's binary_logloss: 0.0475535\nEarly stopping, best iteration is:\n[309]\ttraining's binary_logloss: 0.0449526\tvalid_1's binary_logloss: 0.0475035\nfold n°1\nTraining until validation scores don't improve for 200 rounds.\n[100]\ttraining's binary_logloss: 0.0454224\tvalid_1's binary_logloss: 0.046163\n[200]\ttraining's binary_logloss: 0.0453428\tvalid_1's binary_logloss: 0.0461261\n[300]\ttraining's binary_logloss: 0.0452845\tvalid_1's binary_logloss: 0.0460729\n[400]\ttraining's binary_logloss: 0.0452936\tvalid_1's binary_logloss: 0.0460823\n[500]\ttraining's binary_logloss: 0.0453045\tvalid_1's binary_logloss: 0.0460929\nEarly stopping, best iteration is:\n[309]\ttraining's binary_logloss: 0.0452775\tvalid_1's binary_logloss: 0.0460669\nfold n°2\nTraining until validation scores don't improve for 200 rounds.\n[100]\ttraining's binary_logloss: 0.0456534\tvalid_1's binary_logloss: 0.0448893\n[200]\ttraining's binary_logloss: 0.0455748\tvalid_1's binary_logloss: 0.0447768\n[300]\ttraining's binary_logloss: 0.0455209\tvalid_1's binary_logloss: 0.0447366\n[400]\ttraining's binary_logloss: 0.0455264\tvalid_1's binary_logloss: 0.0447323\n[500]\ttraining's binary_logloss: 0.0455436\tvalid_1's binary_logloss: 0.044739\nEarly stopping, best iteration is:\n[309]\ttraining's binary_logloss: 0.0455165\tvalid_1's binary_logloss: 0.0447288\nfold n°3\nTraining until validation scores don't improve for 200 rounds.\n[100]\ttraining's binary_logloss: 0.0440986\tvalid_1's binary_logloss: 0.0501566\n[200]\ttraining's binary_logloss: 0.0440676\tvalid_1's binary_logloss: 0.0500934\n[300]\ttraining's binary_logloss: 0.0440546\tvalid_1's binary_logloss: 0.0500708\n[400]\ttraining's binary_logloss: 0.0440652\tvalid_1's binary_logloss: 0.0500775\nEarly stopping, best iteration is:\n[294]\ttraining's binary_logloss: 0.0440521\tvalid_1's binary_logloss: 0.0500648\nfold n°4\nTraining until validation scores don't improve for 200 rounds.\n[100]\ttraining's binary_logloss: 0.0452879\tvalid_1's binary_logloss: 0.0455915\n[200]\ttraining's binary_logloss: 0.0452177\tvalid_1's binary_logloss: 0.0455449\nEarly stopping, best iteration is:\n[59]\ttraining's binary_logloss: 0.0452371\tvalid_1's binary_logloss: 0.0455152\nCV score: 0.04678 \nCPU times: user 8min 4s, sys: 6.07 s, total: 8min 10s\nWall time: 2min 13s\n","name":"stdout"}]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"41f9c7d0a52f8d5669704619fdf76ad1451afaa2"},"cell_type":"code","source":"### 'target' is the probability of whether an observation is an outlier\ndf_outlier_prob = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\ndf_outlier_prob[\"target\"] = predictions\ndf_outlier_prob.head()","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"           card_id    target\n0  C_ID_0ab67a22ab  0.043681\n1  C_ID_130fd0cbdd  0.001685\n2  C_ID_b709037bc5  0.006248\n3  C_ID_d27d835a9f  0.001685\n4  C_ID_2b5e3df5c2  0.001685","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>card_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>C_ID_0ab67a22ab</td>\n      <td>0.043681</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>C_ID_130fd0cbdd</td>\n      <td>0.001685</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C_ID_b709037bc5</td>\n      <td>0.006248</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>C_ID_d27d835a9f</td>\n      <td>0.001685</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>C_ID_2b5e3df5c2</td>\n      <td>0.001685</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_uuid":"a6cdbee1c92967a2ecb3924ab58af8d86675df31"},"cell_type":"markdown","source":"# Part 3 Combining Submission:\nSo far so good !\nWe now have three dataset:\n\n1. Best Submission\n2. Prediction Using Model Without Outliers\n3. Probability of Outliers In Test set\n"},{"metadata":{"trusted":true,"_uuid":"d56e431808a9a70a102491ac7d77e4404e312a4b"},"cell_type":"code","source":"# if the test set has the same ratio of outliers as training set, \n# then the numbuer of outliers in test is about: (1.06% outliers in training set)\n123623*0.0106","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"1310.4038"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"40e683026c0e476bae7d15a759b97fa027af977d"},"cell_type":"code","source":"# In case missing some predictable outlier, we choose top 25000 with highest outliers likelyhood.\noutlier_id = pd.DataFrame(df_outlier_prob.sort_values(by='target',ascending = False).head(25000)['card_id'])","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4c9575fad64e1cf9bd38e48fcf5e988b850189f"},"cell_type":"code","source":"best_submission = pd.read_csv('../input/predicting-outliers-to-improve-your-score/3.695.csv')","execution_count":15,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"7c6f4c87221dd9fa0bf2f87a60e46abbe39f7164"},"cell_type":"code","source":"most_likely_liers = best_submission.merge(outlier_id,how='right')\nmost_likely_liers.head()","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"           card_id    target\n0  C_ID_0ab67a22ab -2.502326\n1  C_ID_6d8dba8475 -0.893964\n2  C_ID_7f1041e8e1 -4.872942\n3  C_ID_22e4a47c72  0.393946\n4  C_ID_b54cfad8b2 -0.656266","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>card_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>C_ID_0ab67a22ab</td>\n      <td>-2.502326</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>C_ID_6d8dba8475</td>\n      <td>-0.893964</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C_ID_7f1041e8e1</td>\n      <td>-4.872942</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>C_ID_22e4a47c72</td>\n      <td>0.393946</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>C_ID_b54cfad8b2</td>\n      <td>-0.656266</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"0f1518b58748e9dd2ef3c1d92f161d03f923bac5"},"cell_type":"code","source":"%%time\nfor card_id in most_likely_liers['card_id']:\n    model_without_outliers.loc[model_without_outliers['card_id']==card_id,'target']\\\n    = most_likely_liers.loc[most_likely_liers['card_id']==card_id,'target'].values","execution_count":17,"outputs":[{"output_type":"stream","text":"CPU times: user 16min 57s, sys: 399 ms, total: 16min 57s\nWall time: 16min 57s\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"21e2c892dc716481293cb0b34f6b67be3a912aa9"},"cell_type":"code","source":"model_without_outliers.to_csv(\"combining_submission.csv\", index=False)","execution_count":18,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}